{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from fake_useragent import UserAgent\n",
    "import requests, time, re, math, openpyxl, datetime, os, shutil, psutil, platform, pyautogui, subprocess, webbrowser\n",
    "from tqdm import *\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import urllib.parse\n",
    "import csv\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resource/href_list.txt','r') as hl:\n",
    "    hls = hl.read().splitlines()\n"  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fintech():\n",
    "    url = 'http://www.fintechdb.cn/'\n",
    "    \n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.driver = self.log_in\n",
    "        \n",
    "    def log_in(self):\n",
    "        driver = webdriver.Chrome('D:\\BrowserDriver\\chromedriver')\n",
    "        driver.get(self.url)\n",
    "        time.sleep(1)\n",
    "        login_butten = driver.find_element_by_xpath('//*[@id=\"wa-head\"]/div/div[2]/a')\n",
    "        login_butten.click()\n",
    "        login_butten = driver.find_element_by_xpath('//*[@id=\"wa-login-phone\"]')\n",
    "        login_butten.send_keys(self.username)\n",
    "        login_butten = driver.find_element_by_xpath('//*[@id=\"wa-login-password\"]')\n",
    "        login_butten.send_keys(self.password,Keys.RETURN)\n",
    "        login_butten = driver.find_element_by_xpath('//*[@id=\"wa-lore\"]/li[1]/form/div[4]/button')\n",
    "        login_butten.click()\n",
    "        return driver\n",
    "    \n",
    "    def log_in2(self):\n",
    "        login_butten = driver.find_element_by_xpath('//*[@id=\"wa-login-phone\"]')\n",
    "        login_butten.send_keys(self.username)\n",
    "        login_butten = driver.find_element_by_xpath('//*[@id=\"wa-login-password\"]')\n",
    "        login_butten.send_keys(self.password,Keys.RETURN)\n",
    "        login_butten = driver.find_element_by_xpath('//*[@id=\"wa-lore\"]/li[1]/form/div[4]/button')\n",
    "        login_butten.click()\n",
    "        return driver\n",
    "    \n",
    "    \n",
    "    def target(self, target_location):\n",
    "        tl = driver.find_element_by_link_text(target_location)\n",
    "        tl.click()\n",
    "        time.sleep(random.randint(2,3))\n",
    "        return driver\n",
    "        \n",
    "    def load_more(self):\n",
    "        for i in range(0,4):\n",
    "            try:\n",
    "                load_more = driver.find_element_by_xpath('//*[@id=\"home-load-more\"]')\n",
    "                load_more.click()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "        time.sleep(10)    \n",
    "        return driver\n",
    "    \n",
    "    def page(self):\n",
    "        page_source = driver.page_source\n",
    "        page = bs(page_source,'lxml')\n",
    "        return page\n",
    "     \n",
    "    def updown(self,u):\n",
    "        updown1_box = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[2]/ul/li[3]/a')\n",
    "        updown2_box = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[2]/ul/li[4]/a ')\n",
    "        if u == 1:\n",
    "            updown1_box.click()\n",
    "        else:\n",
    "            updown2_box.click() \n",
    "        time.sleep(1)\n",
    "        return driver\n",
    "    \n",
    "    def order(self,o):\n",
    "        order1_box = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[2]/ul/li[1]/a')\n",
    "        order2_box = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[2]/ul/li[2]/a')\n",
    "        if o == 1:\n",
    "            order1_box.click()\n",
    "        else:\n",
    "            order2_box.click()\n",
    "        time.sleep(1)\n",
    "        return driver\n",
    "    \n",
    "    def write(self,page):\n",
    "        for url in page.find(\"div\",attrs={\"id\": \"company-list\"}).find_all(\"a\"):\n",
    "            href_list.append(url.get('href'))\n",
    "        print(len(set(href_list)), end = \" \")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    def num(self):\n",
    "        num = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[2]/p/span[1]').text\n",
    "        num = re.findall(r'\\d+',num)\n",
    "        time.sleep(1)\n",
    "        return num\n",
    "    \n",
    "    def scrapy(self):\n",
    "        driver = fintech.load_more()\n",
    "        page = fintech.page()\n",
    "        fintech.write(page)\n",
    "    \n",
    "    def scrapy_update(self):\n",
    "        fintech.scrapy()\n",
    "        driver = fintech.updown(0)\n",
    "        fintech.scrapy()\n",
    "        driver = fintech.updown(1)\n",
    "        \n",
    "    def scrapy_update2(self):\n",
    "        fintech.scrapy()\n",
    "        driver = fintech.updown(0)\n",
    "        fintech.scrapy()\n",
    "        driver = fintech.order(0)\n",
    "        fintech.scrapy()\n",
    "        driver = fintech.updown(1)\n",
    "        fintech.scrapy()\n",
    "        driver = fintech.order(1)\n",
    "        \n",
    "    def time_range(self, start_time, end_time):\n",
    "        time_box = driver.find_element_by_xpath('//*[@id=\"fliterTimeStart\"]')\n",
    "        for i in range(1,11):\n",
    "            time_box.send_keys(Keys.BACKSPACE)\n",
    "        time_box.send_keys(start_time)\n",
    "        time_box = driver.find_element_by_xpath('//*[@id=\"fliterTimeEnd\"]')\n",
    "        for i in range(1,11):\n",
    "            time_box.send_keys(Keys.BACKSPACE)\n",
    "        time_box.send_keys(end_time)\n",
    "        return driver\n",
    "       \n",
    "    def adjust(self):\n",
    "        driver = fintech.order(0)\n",
    "        driver = fintech.order(1)\n",
    "        driver = fintech.updown(0)\n",
    "        driver = fintech.updown(1)\n",
    "        time.sleep(1)\n",
    "        return driver\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic = {}\n",
    "fintech = Fintech('18180908264','123456')\n",
    "x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for url in hls[12912:]:\n",
    "    try:\n",
    "        driver = webdriver.Chrome('D:\\BrowserDriver\\chromedriver')\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            fintech.log_in2()\n",
    "        except:\n",
    "            pass   \n",
    "        time.sleep(2)\n",
    "        page = driver.page_source\n",
    "        soup = bs(page,'lxml')\n",
    "        # 开始整理数据\n",
    "        subresult_dic ={}\n",
    "        # 获取标题和简介\n",
    "        try:\n",
    "            title_brief = soup.select('div.wa-company-detail-info > p')\n",
    "            title = title_brief[0].text\n",
    "            brief = title_brief[1].text\n",
    "            subresult_dic['简介'] = str(brief)\n",
    "\n",
    "            # 获取标签\n",
    "            tag_new = []\n",
    "            tags = soup.select('div.wa-company-detail-info > p.wa-product-tags > span')\n",
    "            for tag in tags:\n",
    "                tag_new.append(tag.text)\n",
    "            subresult_dic['标签'] = tag_new\n",
    "\n",
    "            # 获取公司概述表格\n",
    "            table_dic = {}\n",
    "            table_h = []\n",
    "            table_c = []\n",
    "            table_header = soup.find('div', {'class': 'wa-company-detail-content uk-grid uk-grid-small'}).find_all('td',{'class':'uk-width-1-4'}) \n",
    "            for ta in table_header:\n",
    "                table_h.append(ta.text)\n",
    "            table_content = soup.find('div', {'class': 'wa-company-detail-content uk-grid uk-grid-small'}).find_all('td',{'class':'uk-width-3-4'}) \n",
    "            for ta in table_content:\n",
    "                table_c.append(ta.text)\n",
    "            for i in range(0,len(table_header)):\n",
    "                table_dic['%s'%(str(table_h[i]))] = str(table_c[i])\n",
    "            subresult_dic['信息表'] = table_dic\n",
    "\n",
    "\n",
    "            # 获取融资信息\n",
    "            financing_h = []\n",
    "            financing_c = []\n",
    "            financing_dic = {}\n",
    "            finance_header = soup.find('div', {'id': 'wa-product-financing'}).find_all('th') \n",
    "            for fi in finance_header:\n",
    "                financing_h.append(fi.text)\n",
    "            finance_content = soup.find('div', {'id': 'wa-product-financing'}).find_all('td') \n",
    "            for fi in finance_content:\n",
    "                financing_c.append(fi.text)\n",
    "            for i in range(0,len(finance_header)):\n",
    "                financing_dic['%s'%(str(financing_h[i]))] = str(financing_c[i])\n",
    "            subresult_dic['融资'] = financing_dic\n",
    "\n",
    "\n",
    "            # 获取投资信息\n",
    "            investment_h = []\n",
    "            investment_c = []\n",
    "            investment_dic = {}\n",
    "            invest_header = soup.select('div.wa-company-sub-investment-node > h4')\n",
    "            invest_content = soup.select('div.wa-company-sub-investment-node > p')\n",
    "            for in1 in invest_header:\n",
    "                investment_h.append(in1.text)\n",
    "            for in2 in invest_content:\n",
    "                investment_c.append(in2.text)\n",
    "            for i in range(0,len(investment_h)):\n",
    "                investment_dic['%s'%(str(investment_h[i]))] = str(investment_c[i])\n",
    "            subresult_dic['投资'] = financing_dic\n",
    "            result_dic['%s'%(str(title))] = subresult_dic\n",
    "            x += 1\n",
    "            print(len(result_dic), end=\" \")\n",
    "        except:\n",
    "            print('none')\n",
    "        driver.close()\n",
    "    except:\n",
    "        print('gg')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./content_%s.txt'%(len(result_dic)),'w',encoding='utf8') as f:\n",
    "    f.write(json.dumps(result_dic,ensure_ascii=False))\n",
    "    print(\"写入成功\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./content_12655.txt','r',encoding='utf8') as f:\n",
    "    result1 = json.loads(f.read())\n",
    "with open('./content_2361.txt','r',encoding='utf8') as f:\n",
    "    result2 = json.loads(f.read())\n",
    "# with open('./content_259.txt','r',encoding='utf8') as f:\n",
    "#     result3 = json.loads(f.read())\n",
    "# with open('./content_476.txt','r',encoding='utf8') as f:\n",
    "#     result4 = json.loads(f.read())\n",
    "# with open('./content_708.txt','r',encoding='utf8') as f:\n",
    "#     result5 = json.loads(f.read())\n",
    "z = {**result1, **result2}\n",
    "with open('./content_%d.txt'%(len(z)),'w',encoding='utf8') as f:\n",
    "    f.write(json.dumps(z,ensure_ascii=False))\n",
    "    print(\"写入成功\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
